# Blixard: Distributed Service Management System - Complete Project Outline

## ğŸ¯ Project Vision

Blixard is a production-grade **distributed microVM orchestration platform** that provides **reliable, consistent, and fault-tolerant** orchestration of both **long-lived microVMs** and **serverless microVMs** across multiple **NixOS** machines in a cluster. Built with Rust, powered by Raft consensus, and **orchestrating through microvm.nix** as the foundational VM management layer with **Ceph** for distributed storage, it ensures that VM state and storage remain consistent even during network partitions, node failures, and other distributed system challenges.

**Architecture**: Blixard builds on top of microvm.nix, which handles the actual VM creation and lifecycle management through various hypervisors (qemu, firecracker, cloud-hypervisor, etc.). Blixard adds the distributed orchestration layer - consensus, intelligent placement, serverless runtime, and cluster-wide management.

**Think "Kubernetes + AWS Lambda for MicroVMs on NixOS"** - providing enterprise-grade orchestration for:
- **Long-lived VMs**: Traditional services, databases, stateful applications
- **Serverless VMs**: Sub-second startup, event-driven functions, auto-scaling to zero

**ğŸ¯ NixOS-First Design**: Blixard is designed exclusively for NixOS environments, leveraging the Nix package manager, NixOS modules, and microvm.nix for a truly declarative and reproducible virtualization platform.

**âš¡ Serverless Innovation**: Blixard brings AWS Lambda-like serverless computing to on-premise NixOS clusters, with intelligent scheduling that runs functions on any available node based on resource availability, data locality, and network proximity.

## ğŸ—ï¸ System Architecture Overview

### Core Principles
- **NixOS-Native**: Built exclusively for NixOS with deep integration into the Nix ecosystem
- **Consensus-First Design**: Every VM state change goes through Raft consensus
- **MicroVM Isolation**: Strong security boundaries through lightweight virtualization
- **Distributed Storage**: Ceph-backed persistent storage with replication and fault tolerance
- **Declarative Everything**: microvm.nix handles VM definitions, Blixard orchestrates them across the cluster
- **Immutable Infrastructure**: Leverage Nix's immutability for VM configurations and deployments
- **Fault Tolerance**: Survives network partitions, node failures, and storage failures
- **Deterministic Testing**: All functionality verified through simulation testing
- **Zero Downtime**: VM management operations never require cluster downtime
- **Strong Consistency**: All nodes always agree on VM and storage state

### High-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cluster Node            â”‚    â”‚         Cluster Node            â”‚    â”‚         Cluster Node            â”‚
â”‚                                 â”‚    â”‚                                 â”‚    â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    CLI    â”‚  â”‚  Blixard  â”‚   â”‚    â”‚  â”‚    CLI    â”‚  â”‚  Blixard  â”‚   â”‚    â”‚  â”‚    CLI    â”‚  â”‚  Blixard  â”‚   â”‚
â”‚  â”‚ Interface â”‚  â”‚Orchestratorâ”‚   â”‚    â”‚  â”‚ Interface â”‚  â”‚Orchestratorâ”‚   â”‚    â”‚  â”‚ Interface â”‚  â”‚Orchestratorâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚          â”‚    â”‚                       â”‚          â”‚    â”‚                       â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚       microvm.nix Layer     â”‚ â”‚    â”‚  â”‚       microvm.nix Layer     â”‚ â”‚    â”‚  â”‚       microvm.nix Layer     â”‚ â”‚
â”‚  â”‚  (VM Creation & Lifecycle)  â”‚ â”‚    â”‚  â”‚  (VM Creation & Lifecycle)  â”‚ â”‚    â”‚  â”‚  (VM Creation & Lifecycle)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Raft Consensus      â”‚â—„â”¼â”€â”€â”€â”€â”¼â”€â–ºâ”‚         Raft Consensus      â”‚â—„â”¼â”€â”€â”€â”€â”¼â”€â–ºâ”‚         Raft Consensus      â”‚ â”‚
â”‚  â”‚    (VM State + Storage)     â”‚ â”‚    â”‚  â”‚    (VM State + Storage)     â”‚ â”‚    â”‚  â”‚    (VM State + Storage)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚        Ceph Storage         â”‚ â”‚    â”‚  â”‚        Ceph Storage         â”‚ â”‚    â”‚  â”‚        Ceph Storage         â”‚ â”‚
â”‚  â”‚   (OSD + Monitor + MDS)     â”‚â—„â”¼â”€â”€â”€â”€â”¼â”€â–ºâ”‚   (OSD + Monitor + MDS)     â”‚â—„â”¼â”€â”€â”€â”€â”¼â”€â–ºâ”‚   (OSD + Monitor + MDS)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    Hypervisor (KVM)         â”‚ â”‚    â”‚  â”‚    Hypervisor (KVM)         â”‚ â”‚    â”‚  â”‚    Hypervisor (KVM)         â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”    â”‚ â”‚    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”    â”‚ â”‚    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚ VM1 â”‚ â”‚ VM2 â”‚ â”‚ VM3 â”‚    â”‚ â”‚    â”‚  â”‚  â”‚ VM4 â”‚ â”‚ VM5 â”‚ â”‚ VM6 â”‚    â”‚ â”‚    â”‚  â”‚  â”‚ VM7 â”‚ â”‚ VM8 â”‚ â”‚ VM9 â”‚    â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    â”‚ â”‚    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    â”‚ â”‚    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                       â”‚                                       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚           Network Layer                â”‚
                            â”‚  â€¢ Service Discovery (Tailscale)      â”‚
                            â”‚  â€¢ Virtual Networking (VXLANs)        â”‚
                            â”‚  â€¢ Load Balancing & Ingress           â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ What Blixard Will Do Once Complete

### Primary Capabilities

#### 1. **Distributed MicroVM Orchestration**
- **Dual VM Models**:
  - **Long-lived VMs**: Traditional services, databases, stateful applications with persistent lifecycle
  - **Serverless VMs**: Event-driven functions with <100ms cold start, auto-scaling to zero
- **Intelligent Scheduling**: 
  - Long-lived VMs: Placement based on resource requirements and affinity rules
  - Serverless VMs: Dynamic placement on any available node based on:
    - Current resource availability (CPU, memory, network)
    - Data locality (run near the data being processed)
    - Network proximity (minimize latency to event sources)
    - Node specialization (GPU nodes for ML functions, etc.)
- **VM Lifecycle Management**: Orchestrate microvm.nix VMs across cluster nodes
- **Declarative Configuration**: VMs defined through microvm.nix, orchestrated by Blixard
- **Real-time Monitoring**: VM health, resource usage, and performance metrics
- **Live Migration**: Move long-lived VMs between nodes with zero downtime
- **Resource Management**: CPU, memory, disk, and network allocation with overcommit for serverless
- **Security Isolation**: Strong boundaries through hardware virtualization (via microvm.nix)
- **Multi-Hypervisor Support**: All microvm.nix hypervisors - Firecracker (serverless), Cloud Hypervisor, QEMU, crosvm, kvmtool, stratovirt, alioth

#### 2. **Distributed Storage with Ceph**
- **Block Storage (RBD)**: Persistent volumes for VM disks with replication
- **Object Storage (RADOS)**: Application data with versioning and lifecycle management
- **File Storage (CephFS)**: Shared filesystems across VMs
- **Storage Orchestration**: Automatic pool creation, placement groups, and rebalancing
- **Data Durability**: Configurable replication and erasure coding
- **Performance Tiers**: SSD/NVMe for hot data, HDD for cold storage
- **Snapshot Management**: Point-in-time snapshots and clones

#### 3. **High Availability and Fault Tolerance**
- **VM Automatic Failover**: VMs automatically restart on healthy nodes
- **Storage Redundancy**: Ceph replication protects against disk/node failures
- **Split-Brain Prevention**: Raft consensus for VM state, Ceph quorum for storage
- **Network Partition Tolerance**: VMs continue running, storage maintains consistency
- **Live Migration**: Move VMs away from failing hardware
- **Disaster Recovery**: Cross-region Ceph replication and VM image backup
- **Rolling Updates**: Update hypervisors without VM downtime

#### 4. **Developer Experience**
- **Declarative VM Definitions**: microvm.nix configurations for both long-lived and serverless VMs
- **Function Development**: Simple Nix expressions for serverless functions
  ```nix
  # serverless-function.nix
  { event, context }: { 
    statusCode = 200;
    body = "Hello from NixOS serverless! Event: ${event.type}";
  }
  ```
- **CLI Interface**: Kubectl-like commands + Lambda-like function management
- **GitOps Workflow**: Infrastructure as Code with automatic deployment
- **Rich Status Information**: VM metrics, function invocations, cold/warm starts
- **Local Testing**: Test serverless functions locally before deployment
- **Development Environments**: Instant dev VMs with shared storage
- **Function Triggers**: HTTP endpoints, event streams, scheduled invocations

### Example Usage Scenarios

#### Scenario 0: Serverless Functions (Lambda-like)
```bash
# Define a serverless function
cat > image-processor.nix <<EOF
{ pkgs, ... }: {
  # Minimal microVM config for fast startup
  microvm = {
    mem = 128;  # Can be as low as needed
    vcpu = 1;
    bootTime = "50ms";  # Optimized for speed
  };
  
  # Function handler
  handler = { event, context }: 
    let
      image = pkgs.imagemagick.processImage event.imagePath;
    in {
      statusCode = 200;
      body = {
        processed = true;
        outputPath = image.path;
      };
    };
}
EOF

# Deploy serverless function
blixard function create image-processor --config image-processor.nix \
  --memory 128M --timeout 30s --concurrent-executions 100

# Function automatically scales to zero when idle
blixard function list
# Output: image-processor (0 instances running, 0 warm)

# Invoke function (cold start ~80ms)
blixard function invoke image-processor --data '{"imagePath": "/data/input.jpg"}'
# Function runs on optimal node based on:
# - Available CPU/memory
# - Proximity to /data/input.jpg if using Ceph
# - Network latency to caller

# After invocation, function stays warm briefly
blixard function list  
# Output: image-processor (1 instance warm on nixos-node3)

# Create HTTP trigger
blixard function trigger image-processor --http --path /api/process-image
# Now accessible at: https://cluster.local/api/process-image

# View function metrics
blixard function metrics image-processor
# Output: Invocations: 1.2k/hour, Avg cold start: 82ms, Avg duration: 450ms
```

#### Scenario 1: Web Application Deployment (NixOS-Native)
```bash
# Create VM definition using NixOS modules
cat > webapp.nix <<EOF
{ pkgs, modulesPath, ... }: {
  imports = [ (modulesPath + "/profiles/qemu-guest.nix") ];
  
  microvm = {
    mem = 512;
    vcpu = 2;
    balloonMem = 256;
    shares = [{
      source = "/nix/store";
      mountPoint = "/nix/.ro-store";
      tag = "ro-store";
      proto = "virtiofs";
    }];
  };
  
  # Pure NixOS service configuration
  services.nginx = {
    enable = true;
    virtualHosts.default = {
      root = "/var/www";
      locations."/".index = "index.html";
    };
  };
  
  networking.firewall.allowedTCPPorts = [ 80 ];
  
  # Declarative user management
  users.users.webapp = {
    isNormalUser = true;
    home = "/var/www";
  };
}
EOF

# Deploy across NixOS cluster with Ceph storage
blixard vm create webapp --config webapp.nix --replicas 3 \
  --storage-pool ssd-pool --network web-vlan
blixard vm start webapp
blixard vm status webapp
# Output: webapp VMs running on [nixos-node1, nixos-node2, nixos-node3]

# Scale with automatic placement
blixard vm scale webapp --replicas 5
# Creates VMs using Nix store sharing for efficiency

# Rolling update with NixOS generations
blixard vm update webapp --config webapp-v2.nix --strategy rolling
# Uses NixOS atomic upgrades + live migration
```

#### Scenario 2: Database Cluster Management (NixOS-Native)
```bash
# Create PostgreSQL cluster using NixOS modules
cat > postgres.nix <<EOF
{ pkgs, config, modulesPath, ... }: {
  imports = [ (modulesPath + "/profiles/qemu-guest.nix") ];
  
  microvm = {
    mem = 2048;
    vcpu = 4;
    shares = [{
      source = "/nix/store";
      mountPoint = "/nix/.ro-store";
      tag = "ro-store";
      proto = "virtiofs";
    }];
  };
  
  # Pure NixOS PostgreSQL configuration
  services.postgresql = {
    enable = true;
    package = pkgs.postgresql15;
    dataDir = "/var/lib/postgresql/15";
    settings = {
      shared_preload_libraries = [ "pg_stat_statements" ];
      max_connections = 200;
      shared_buffers = "256MB";
    };
    authentication = '';
      local all all trust
      host all all 0.0.0.0/0 md5
    '';
  };
  
  # Declarative database setup
  services.postgresql.initialScript = pkgs.writeText "init.sql" ''
    CREATE DATABASE app_db;
    CREATE USER app_user WITH PASSWORD 'secure_password';
    GRANT ALL PRIVILEGES ON DATABASE app_db TO app_user;
  '';
  
  # Ceph RBD mount for persistent data
  fileSystems."/var/lib/postgresql" = {
    device = "ceph-pool/postgres-vol";
    fsType = "ext4";
    options = [ "defaults" "noatime" ];
  };
  
  networking.firewall.allowedTCPPorts = [ 5432 ];
}
EOF

# Deploy with Ceph replication and NixOS reproducibility
blixard vm create postgres --config postgres.nix --replicas 3 \
  --storage-class replicated --storage-size 100G \
  --primary-election-policy "oldest-first"
blixard vm start postgres

# NixOS atomic failover with storage migration
# When primary VM fails, Blixard:
# 1. Promotes secondary VM to primary using NixOS activation
# 2. Mounts Ceph volume on new primary
# 3. Updates load balancer configuration
blixard vm status postgres
# Output: postgres primary=nixos-postgres-2, replicas=[nixos-postgres-2, nixos-postgres-3]
```

#### Scenario 3: Complete Microservices Platform (Mixed Long-lived + Serverless)
```bash
# Deploy platform mixing long-lived services and serverless functions
cat > platform.yaml <<EOF
apiVersion: blixard.dev/v1
kind: Platform
metadata:
  name: ecommerce-platform
spec:
  # Long-lived services
  vms:
    - name: postgres-db
      type: long-lived
      config: ./configs/postgres.nix
      replicas: 3
      storage: { class: "ssd", size: "100G" }
      placement: { nodeSelector: { storage: "fast" } }
    
    - name: redis-cache
      type: long-lived  
      config: ./configs/redis.nix
      replicas: 2
      memory: 4096
      placement: { spreadAcross: "availability-zones" }
  
  # Serverless functions
  functions:
    - name: order-processor
      config: ./functions/order-processor.nix
      memory: 256
      timeout: 60s
      triggers:
        - type: queue
          source: orders-queue
      scaling:
        minInstances: 0  # Scale to zero
        maxInstances: 100
        targetConcurrency: 1
    
    - name: image-resizer
      config: ./functions/image-resizer.nix  
      memory: 512
      timeout: 30s
      triggers:
        - type: http
          path: /api/resize
      placement:
        # Run on any node, prefer those with GPU
        nodeAffinity: { preferred: { gpu: "available" } }
    
    - name: pdf-generator
      config: ./functions/pdf-generator.nix
      memory: 1024
      timeout: 120s
      triggers:
        - type: event
          source: invoice-created
      # This function can run on ANY node in the cluster
      # Blixard will choose optimal placement at runtime
EOF

blixard apply -f platform.yaml
# Creates long-lived VMs with fixed placement AND serverless functions

# Monitor mixed platform
blixard dashboard --stack ecommerce-platform
# Shows: 
# - Long-lived VMs: postgres (3/3 running), redis (2/2 running)
# - Serverless functions: order-processor (0 running, 2 warm), 
#   image-resizer (5 running on nodes 2,3,5), pdf-generator (0 running)

# Watch serverless scaling in action
blixard function watch order-processor
# Shows real-time scaling as orders come in, placement decisions,
# and which nodes are selected for each invocation
```

## ğŸ›ï¸ Technical Architecture

### Integration with microvm.nix

Blixard operates as an orchestration layer on top of microvm.nix:

1. **microvm.nix handles**:
   - VM creation and lifecycle (start, stop, destroy)
   - Hypervisor abstraction (qemu, firecracker, cloud-hypervisor, crosvm, etc.)
   - Resource allocation (CPU, memory, disk)
   - Network interface creation
   - Filesystem sharing and virtio devices

2. **Blixard adds**:
   - Distributed consensus for VM state across nodes
   - Intelligent placement and scheduling decisions
   - Serverless function runtime and warm pools
   - Any-node execution with data locality awareness
   - Cluster-wide VM management and monitoring
   - Integration with Ceph for distributed storage
   - Live migration orchestration between nodes

3. **Workflow**:
   ```
   User Request â†’ Blixard CLI â†’ Raft Consensus â†’ Placement Decision
        â†“
   Selected Node â†’ microvm.nix API â†’ Hypervisor â†’ Running VM
        â†“
   Blixard monitors and manages the VM lifecycle across the cluster
   ```

### Core Components

#### 1. **Consensus Layer** (`src/consensus/`)
- **Raft Implementation**: Based on tikv/raft-rs for battle-tested consensus
- **VM State Replication**: All VM lifecycle operations replicated across cluster
- **Storage Coordination**: Coordinate with Ceph for consistent VM placement
- **Leader Election**: Automatic leader selection with failure detection
- **Snapshot Management**: Efficient VM state and cluster metadata transfer

```rust
// Example: VM operation through consensus
let operation = VMOperation::Create {
    vm_name: "webapp-1".to_string(),
    config: VMConfig {
        memory_mb: 512,
        vcpus: 2,
        nix_config: "./webapp.nix".to_string(),
        storage: StorageSpec {
            pool: "ssd-pool".to_string(),
            size_gb: 20,
            replicas: 3,
        },
    },
    target_node: None, // Auto-placement
};

// This goes through Raft consensus before execution
cluster.propose(operation).await?;
```

#### 2. **Storage Layer** (`src/storage/`)
- **Ceph Integration**: RADOS, RBD, and CephFS management
- **VM State Store**: VM definitions, current state, and placement decisions
- **Storage Orchestration**: Pool creation, PG management, rebalancing
- **Configuration Storage**: microvm.nix configs and cluster settings
- **Audit Log**: Complete history of VM and storage operations
- **Backup/Restore**: VM snapshots and cross-region replication

```rust
// VM state is strongly consistent across cluster
pub struct VMState {
    pub name: String,
    pub status: VMStatus,  // Creating, Running, Migrating, Stopped, Failed
    pub node_id: Option<String>,
    pub hypervisor: HypervisorType,  // Firecracker, CloudHypervisor, QEMU
    pub config: VMConfig,
    pub storage: StorageAllocation,
    pub network: NetworkConfig,
    pub health: HealthStatus,
    pub metrics: VMMetrics,
    pub last_updated: SystemTime,
}

pub struct StorageAllocation {
    pub rbd_volumes: Vec<RBDVolume>,
    pub cephfs_mounts: Vec<CephFSMount>,
    pub object_buckets: Vec<ObjectBucket>,
}
```

#### 3. **MicroVM Management** (`src/microvm/`)
- **microvm.nix Integration**:
  - Blixard calls microvm.nix to create/manage VMs on each node
  - Translates high-level orchestration decisions to microvm.nix operations
  - Monitors VM state through microvm.nix interfaces
- **Dual VM Types**:
  - **Long-lived VMs**: Traditional lifecycle via microvm.nix
  - **Serverless VMs**: Rapid lifecycle with pre-configured microvm.nix templates
- **Hypervisor Selection**: 
  - Uses microvm.nix's hypervisor abstraction
  - Firecracker (preferred for serverless - fastest boot)
  - All microvm.nix supported hypervisors available
- **Serverless Runtime**:
  - Sub-100ms cold starts with Firecracker
  - Warm pool management for frequently used functions
  - Automatic instance recycling and cleanup
- **Smart Placement Engine**:
  - Long-lived VMs: Respect affinity/anti-affinity rules
  - Serverless VMs: Dynamic placement on ANY suitable node based on:
    - Real-time resource availability
    - Data locality (Ceph storage proximity)
    - Network topology (minimize latency)
    - Node capabilities (GPU, high-memory, etc.)
- **Resource Management**: 
  - Fixed allocation for long-lived VMs
  - Overcommit and bursting for serverless functions
- **Live Migration**: Zero-downtime movement (long-lived VMs only)
- **Health Monitoring**: VM health checks, function error rates, cold start metrics

```rust
// Example: Dual VM configurations
pub enum VMConfig {
    LongLived(LongLivedVMConfig),
    Serverless(ServerlessVMConfig),
}

pub struct LongLivedVMConfig {
    pub name: String,
    pub microvm_nix_flake: String,      // microvm.nix flake reference
    pub microvm_nix_config: PathBuf,    // Path to microvm.nix configuration
    pub hypervisor: HypervisorType,     // Which microvm.nix hypervisor to use
    pub resources: FixedVMResources,
    pub storage: StorageRequirements,
    pub network: NetworkRequirements,
    pub dependencies: Vec<String>,
    pub health_check: HealthCheckConfig,
    pub placement: PlacementPolicy,
}

pub struct ServerlessVMConfig {
    pub name: String,
    pub microvm_nix_template: String,  // Pre-built microvm.nix template for fast boot
    pub handler_path: PathBuf,         // Nix function definition
    pub memory_mb: u32,                // Max memory
    pub timeout_ms: u32,               // Max execution time
    pub triggers: Vec<TriggerConfig>,  // HTTP, queue, event, cron
    pub scaling: ScalingPolicy,
    pub placement: DynamicPlacement,   // Flexible placement
}

pub struct DynamicPlacement {
    pub strategy: PlacementStrategy,
    pub constraints: Vec<Constraint>,
    pub preferences: Vec<Preference>,
}

pub enum PlacementStrategy {
    // Run on ANY available node
    BestFit,           // Optimal resource utilization
    DataLocality,      // Near the data being processed  
    NetworkProximity,  // Minimize latency to caller
    LoadBalanced,      // Spread across cluster
    // Can combine strategies
    Combined(Vec<PlacementStrategy>),
}

pub struct ScalingPolicy {
    pub min_instances: u32,     // Can be 0 for scale-to-zero
    pub max_instances: u32,     // Max concurrent executions
    pub target_concurrency: u32, // Invocations per instance
    pub scale_down_delay_ms: u32, // Keep warm duration
}
```

#### 4. **Networking Layer** (`src/network/`)
- **Virtual Networking**: VLAN, VXLAN, and bridge management for VM isolation
- **gRPC Communication**: Efficient inter-node cluster communication
- **Service Discovery**: Automatic node discovery via Tailscale mesh networking
- **Load Balancing**: L4/L7 load balancing for VM services
- **Network Partition Handling**: Graceful degradation with storage quorum
- **Ingress/Egress**: Traffic routing and firewall management
- **Multi-tenancy**: Network isolation between different VM workloads

#### 5. **CLI Interface** (`src/cli/`)
- **VM Management**: kubectl-like commands for VM lifecycle operations
- **Storage Operations**: Ceph pool, volume, and filesystem management
- **Configuration Management**: Validate and distribute microvm.nix configurations
- **Interactive Mode**: REPL-style interface for exploration and debugging
- **GitOps Integration**: Apply configurations from Git repositories
- **Output Formatting**: JSON, YAML, table formats with filtering
- **Autocomplete**: Shell completion for all commands and resources

```bash
# Blixard orchestrates microvm.nix VMs across the cluster
# The nginx.nix file is a standard microvm.nix configuration
blixard vm create nginx --microvm-config nginx.nix --replicas 3 \
  --storage-class ssd
# Blixard selects nodes and calls microvm.nix on each to create VMs

blixard vm start nginx
# Triggers 'microvm -c nginx.nix' on selected nodes

blixard vm status nginx --format json
blixard vm logs nginx --follow --tail 100
blixard vm migrate nginx-1 --target-node node2 --live

# Storage management
blixard storage pool create ssd-pool --type replicated --size 1TB
blixard storage volume create app-data --pool ssd-pool --size 50G
blixard storage fs create shared-data --pool ssd-pool

# Cluster operations
blixard cluster status --verbose
blixard cluster join --node-id node4 --address 10.0.0.4
blixard cluster storage add-osd /dev/nvme1n1 --weight 1.0
```

### Advanced Features

#### 1. **Intelligent VM Scheduling**
- **Dual Scheduling Modes**:
  - **Long-lived VMs**: Traditional constraint-based scheduling
  - **Serverless VMs**: Dynamic best-fit scheduling across ANY available node
- **Resource-Aware Placement**: 
  - Long-lived: Reserved resources based on requirements
  - Serverless: Opportunistic placement with overcommit
- **Serverless Placement Algorithm**:
  ```rust
  // Serverless functions can run on ANY node
  // Blixard evaluates all nodes in real-time:
  fn select_node_for_function(func: &ServerlessFunction) -> NodeId {
    cluster.nodes()
      .filter(|n| n.available_memory() >= func.memory)
      .filter(|n| n.matches_constraints(&func.constraints))
      .max_by_key(|n| {
        let resource_score = n.available_resources_score();
        let data_locality_score = n.data_proximity_score(&func.data);
        let network_score = n.network_proximity_score(&func.caller);
        let specialization_score = n.hardware_match_score(&func.needs);
        
        // Weighted combination for optimal placement
        resource_score * 0.3 + 
        data_locality_score * 0.3 +
        network_score * 0.2 +
        specialization_score * 0.2
      })
  }
  ```
- **Storage Locality**: 
  - Long-lived VMs: Place near persistent Ceph volumes
  - Serverless: Run near the data being processed
- **Hardware Specialization**: 
  - GPU nodes for ML inference functions
  - High-memory nodes for data processing
  - Edge nodes for IoT event handlers
- **Zero-Queue Architecture**: Serverless functions scheduled immediately on best available node

```bash
# Long-lived VM scheduling (traditional)
blixard vm create database --config postgres.nix \
  --type long-lived \
  --constraint "storage=nvme,memory>=32GB" \
  --anti-affinity-group webapp \
  --storage-locality required

# Serverless function deployment (dynamic placement)
blixard function create image-ai --config image-ai.nix \
  --memory 2048M --timeout 60s \
  --placement-strategy best-fit,data-locality \
  --prefer "gpu=available" \
  --max-concurrent 50
# Function will run on ANY suitable node at invocation time

# ML inference function that runs anywhere with GPU
blixard function create ml-inference --config inference.nix \
  --memory 4096M \
  --constraint "gpu.memory>=8GB" \
  --placement-strategy network-proximity
# Each invocation finds optimal GPU node based on caller location

# Data processing function with Ceph locality
blixard function create etl-processor --config etl.nix \
  --memory 8192M \
  --placement-strategy data-locality \
  --data-hint "ceph-pool:raw-data"
# Runs on nodes closest to the Ceph OSDs containing the data

# Edge function for IoT events  
blixard function create iot-handler --config iot.nix \
  --memory 64M --timeout 5s \
  --placement-strategy network-proximity \
  --prefer "location=edge"
# Automatically runs on edge nodes closest to IoT devices
```

#### 2. **Live Migration and Zero-Downtime Updates**
- **Live VM Migration**: Move running VMs between nodes without downtime
- **Rolling Updates**: Update VM configurations with live migration
- **Blue-Green Deployments**: Parallel VM environments for safe updates
- **Canary Deployments**: Gradual rollout with automatic traffic shifting
- **Rollback Capability**: Instant VM snapshot restoration
- **Health-Aware Updates**: Only proceed if VMs remain healthy and responsive

```bash
# Rolling update with live migration
blixard vm update webapp --config webapp-v2.nix \
  --strategy rolling --max-unavailable 1 \
  --live-migrate --health-check-timeout 30s \
  --rollback-on-failure

# Blue-green deployment
blixard vm deploy webapp-green --config webapp-v2.nix \
  --parallel-to webapp-blue --traffic-split 10%

# Canary with automatic promotion
blixard vm canary webapp --config webapp-v2.nix \
  --canary-percent 5% --success-threshold 99.9% \
  --auto-promote --monitor-duration 10m
```

#### 3. **Comprehensive Monitoring and Observability**
- **VM Metrics**: CPU, memory, disk I/O, network per VM with sub-second granularity
- **Storage Metrics**: Ceph performance, utilization, replication status
- **Hypervisor Metrics**: KVM/Firecracker performance and resource usage
- **Network Telemetry**: Virtual network flows, bandwidth, latency
- **Log Aggregation**: Centralized logging from VMs and infrastructure
- **Distributed Tracing**: Request tracing across VM boundaries
- **Alerting Integration**: Prometheus, Grafana, webhook integrations

```bash
# Comprehensive monitoring capabilities
blixard vm metrics webapp --time-range 1h --granularity 1s
blixard storage metrics --pool ssd-pool --show-osd-breakdown
blixard network flows --vm webapp --protocol tcp --ports 80,443
blixard logs --vm webapp --aggregate --since 10m --filter "ERROR"

# Interactive dashboards
blixard dashboard --port 8080  # Web-based cluster dashboard
blixard dashboard storage      # Ceph health and performance
blixard dashboard network      # Virtual network topology
```

#### 4. **Advanced Backup and Disaster Recovery**
- **VM Snapshots**: Instant point-in-time VM state capture
- **Incremental Backups**: Efficient Ceph RBD differential backups
- **Cross-Region Replication**: Automatic Ceph mirroring for disaster recovery
- **Geo-Distributed Clusters**: Multi-site deployments with automatic failover
- **Data Migration**: Live VM and storage migration between clusters
- **Compliance**: Immutable backups with retention policies

```bash
# Advanced backup and recovery
blixard vm snapshot webapp --name pre-update-$(date +%Y%m%d)
blixard storage backup --pool app-data --destination geo-backup \
  --schedule daily --retention 30d

# Cross-region disaster recovery
blixard cluster replicate --destination region-west \
  --replication-mode async --rpo 5m

# Point-in-time recovery
blixard vm restore webapp --snapshot pre-update-20241209 \
  --new-name webapp-restored
blixard storage restore app-data --point-in-time "2024-12-09 15:30:00"
```

## ğŸ§ª Testing and Quality Assurance

### Testing Strategy

#### 1. **Deterministic Simulation Testing**
- **Complete MicroVM Simulation**: Test entire VM orchestration in simulated environment
- **Storage Simulation**: Simulate Ceph cluster behavior, OSD failures, network partitions
- **Hypervisor Simulation**: Mock Firecracker/KVM operations for deterministic testing
- **Network Partition Simulation**: Verify VM and storage behavior during network splits
- **Node Failure Simulation**: Test VM live migration and Ceph rebalancing
- **Time Acceleration**: Run months of VM lifecycle operations in minutes

```rust
#[madsim::test]
async fn test_vm_cluster_partition_tolerance() {
    let mut sim = create_simulation(5).await;
    
    // Create VMs with Ceph storage across cluster
    sim.create_vm_stack("webapp", VMConfig {
        replicas: 3,
        storage: CephStorageSpec {
            pool: "ssd-pool",
            replication: 3,
            size_gb: 20,
        },
        hypervisor: HypervisorType::Firecracker,
    }).await;
    
    // Simulate network partition
    sim.partition_nodes([0, 1], [2, 3, 4]).await;
    
    // Verify VM consensus safety and Ceph quorum
    assert!(sim.verify_no_vm_split_brain().await);
    assert!(sim.verify_ceph_quorum_maintained().await);
    assert!(sim.verify_vm_availability("webapp").await);
    
    // Simulate live migration during partition
    sim.trigger_live_migration("webapp-1", "node4").await;
    assert!(sim.verify_vm_migrated_successfully().await);
    
    // Heal partition and verify convergence
    sim.heal_partition().await;
    assert!(sim.verify_cluster_convergence().await);
    assert!(sim.verify_ceph_rebalancing_complete().await);
}
```

#### 2. **Property-Based Testing**
- **VM State Invariants**: Verify VMs never enter invalid states during lifecycle operations
- **Storage Consistency**: Ensure Ceph data consistency across all operations
- **Consensus Safety**: Ensure no two nodes disagree on VM or storage state
- **Resource Constraints**: Verify VM CPU/memory limits and Ceph placement rules
- **Live Migration Safety**: Ensure VM state preservation during migrations
- **Storage Replication**: Verify Ceph replication factor maintenance

```rust
proptest! {
    #[test]
    fn vm_operations_maintain_invariants(
        operations in vec(any::<VMOperation>(), 1..100)
    ) {
        let cluster = TestCluster::new(3);
        
        for op in operations {
            cluster.apply_vm_operation(op)?;
            
            // VM and storage invariants that must always hold
            prop_assert!(cluster.all_nodes_agree_on_vm_state());
            prop_assert!(cluster.ceph_data_consistent());
            prop_assert!(cluster.no_vm_resource_violations());
            prop_assert!(cluster.storage_replication_satisfied());
            prop_assert!(cluster.vm_dependencies_satisfied());
            prop_assert!(cluster.no_orphaned_storage());
        }
    }
    
    #[test]
    fn live_migration_preserves_vm_state(
        vm_configs in vec(arbitrary_vm_config(), 1..10),
        migration_sequence in vec(migration_operation(), 1..20)
    ) {
        let cluster = TestCluster::new(5);
        
        // Create VMs
        for config in vm_configs {
            cluster.create_vm(config)?;
        }
        
        // Perform migration operations
        for migration in migration_sequence {
            let pre_state = cluster.capture_vm_state(&migration.vm_name);
            cluster.live_migrate(migration)?;
            let post_state = cluster.capture_vm_state(&migration.vm_name);
            
            // VM state must be preserved during migration
            prop_assert_eq!(pre_state.memory_contents, post_state.memory_contents);
            prop_assert_eq!(pre_state.cpu_state, post_state.cpu_state);
            prop_assert_eq!(pre_state.network_connections, post_state.network_connections);
        }
    }
}
```

#### 3. **Chaos Engineering**
- **VM Host Failures**: Kill hypervisor hosts during VM operations
- **Storage Chaos**: Simulate Ceph OSD failures, disk corruption, network splits
- **Hypervisor Chaos**: Crash Firecracker/KVM processes during VM lifecycle
- **Live Migration Failures**: Interrupt migrations at various stages
- **Network Chaos**: Introduce packet loss, delays, and VLAN partitions
- **Resource Pressure**: Simulate CPU, memory, disk, and network exhaustion
- **Clock Skew**: Test VM scheduling and storage operations with time drift

#### 4. **Performance Testing**
- **Throughput Testing**: Maximum operations per second
- **Latency Testing**: Response time under various loads
- **Scalability Testing**: Performance with increasing cluster size
- **Resource Usage**: Memory and CPU consumption profiling

### Quality Metrics

#### Target Performance Characteristics
- **Consensus Latency**: < 10ms for local cluster operations
- **Service Start Time**: < 5 seconds for typical services
- **Cluster Convergence**: < 30 seconds after network partition healing
- **Throughput**: > 1000 service operations per second
- **Availability**: 99.9% uptime for properly configured clusters

#### Reliability Requirements
- **Zero Split-Brain**: Mathematically impossible under correct operation
- **No Data Loss**: All committed operations are durable
- **Automatic Recovery**: < 60 seconds to detect and recover from failures
- **Partition Tolerance**: Continues operating with minority partitions

## ğŸš€ Deployment and Operations

### Installation and Setup (NixOS-Only)

#### 1. **NixOS Single-Node Setup**
```bash
# Add Blixard to your NixOS configuration
# /etc/nixos/configuration.nix
{ config, pkgs, ... }: {
  # Enable Blixard
  services.blixard = {
    enable = true;
    mode = "single-node";
    storage.ceph = {
      enable = true;
      osds = [ "/dev/sdb" ];
    };
  };
  
  # Enable required virtualization
  virtualisation = {
    libvirtd.enable = true;
    kvmgt.enable = true;
  };
  
  # Enable microvm support
  microvm.host.enable = true;
  
  # Install Blixard CLI
  environment.systemPackages = with pkgs; [
    blixard
  ];
}

# Rebuild NixOS
sudo nixos-rebuild switch

# Start managing VMs with microvm.nix
blixard vm create nginx --config examples/nginx.nix --storage-size 10G
blixard vm start nginx
```

#### 2. **NixOS Multi-Node Cluster**
```bash
# Primary node NixOS configuration
# /etc/nixos/configuration.nix
{ config, pkgs, ... }: {
  services.blixard = {
    enable = true;
    mode = "cluster-primary";
    nodeId = "nixos-node1";
    storage.ceph = {
      enable = true;
      role = [ "mon" "osd" "mgr" ];
      osds = [ "/dev/nvme0n1" ];
    };
  };
  
  networking.hostName = "nixos-node1";
  virtualisation.microvm.host.enable = true;
}

# Additional nodes NixOS configuration
# /etc/nixos/configuration.nix
{ config, pkgs, ... }: {
  services.blixard = {
    enable = true;
    mode = "cluster-join";
    nodeId = "nixos-node2";
    primaryNode = "nixos-node1.local";
    storage.ceph = {
      enable = true;
      role = [ "osd" "mds" ];
      osds = [ "/dev/nvme0n1" ];
    };
  };
  
  networking.hostName = "nixos-node2";
  virtualisation.microvm.host.enable = true;
}

# Deploy across all nodes
sudo nixos-rebuild switch

# Verify cluster formation
blixard cluster status
blixard storage status
blixard storage pool create ssd-pool --type replicated --size 3
```

#### 3. **Production NixOS Deployment**
```nix
# /etc/nixos/configuration.nix (Production)
{ config, pkgs, ... }: {
  services.blixard = {
    enable = true;
    mode = "production";
    
    cluster = {
      discovery.method = "tailscale";
      discovery.tailscale.authkey = "$TAILSCALE_KEY";
    };
    
    storage.ceph = {
      enable = true;
      replicationFactor = 3;
      pgNum = 128;
      defaultPool = "ssd-pool";
    };
    
    monitoring = {
      prometheus = {
        enable = true;
        port = 9090;
      };
      ceph.dashboard = true;
      grafana.enable = true;
    };
  };
  
  # NixOS security hardening
  security.polkit.enable = true;
  virtualisation.libvirtd.qemu.swtpm.enable = true;
  
  # Automatic updates for security
  system.autoUpgrade = {
    enable = true;
    channel = "nixos-23.11";
  };
}

# Deploy via NixOps, deploy-rs, or manual rebuild
sudo nixos-rebuild switch
```

### Configuration Management

#### Configuration Files
```toml
# /etc/blixard/config.toml
[cluster]
node_id = "node1"
data_dir = "/var/lib/blixard"
log_level = "info"

[discovery]
method = "tailscale"
port = 7946

[consensus]
election_timeout = "5s"
heartbeat_interval = "1s"
snapshot_threshold = 1000

[microvm]
default_hypervisor = "firecracker"
default_memory_mb = 512
default_vcpus = 1
max_vms_per_node = 100
nix_config_dir = "/etc/blixard/vm-configs"

[storage.ceph]
config_file = "/etc/ceph/ceph.conf"
default_pool = "ssd-pool"
replication_factor = 3
pg_num = 128
min_size = 2

[storage.pools.ssd-pool]
type = "replicated"
size = 3
min_size = 2
rule = "ssd_rule"

[storage.pools.hdd-pool]
type = "erasure"
k = 4
m = 2
rule = "hdd_rule"

[network]
vlan_range = "100-200"
bridge_name = "blixard0"
default_gateway = "10.0.0.1"
dhcp_pool = "10.0.1.0/24"

[monitoring]
metrics_enabled = true
metrics_port = 9090
ceph_dashboard = true
log_aggregation = true
grafana_enabled = true
```

### Operational Procedures

#### 1. **Adding New Nodes**
```bash
# On existing cluster node
blixard cluster add-node --node-id node4 --address 10.0.0.4 \
  --roles "hypervisor,ceph-osd"

# On new node (with storage)
blixard init --cluster-join cluster.example.com --node-id node4 \
  --ceph-osd /dev/nvme0n1 --osd-weight 1.0

# Verify integration and rebalancing
blixard cluster status --verbose
blixard storage status --show-rebalancing
```

#### 2. **Removing Nodes**
```bash
# Graceful node removal with VM and storage migration
blixard cluster remove-node node4 --migrate-vms --drain-osds

# Wait for rebalancing to complete
blixard storage wait-for-rebalance --timeout 30m

# Emergency removal (node is down)
blixard cluster force-remove-node node4 --mark-osds-down
blixard storage osd crush remove osd.4
```

#### 3. **Upgrading Blixard**
```bash
# Rolling upgrade with VM live migration (requires 3+ nodes)
blixard cluster upgrade --version v2.0.0 --strategy rolling \
  --migrate-vms --ceph-safe-mode

# Manual upgrade with maintenance mode
blixard cluster maintenance-mode enable
blixard storage set-flag noout  # Prevent Ceph rebalancing
# Upgrade Blixard binary on all nodes
blixard storage unset-flag noout
blixard cluster maintenance-mode disable
```

## ğŸ“Š Monitoring and Observability

### Metrics and Monitoring

#### System Metrics
- **Cluster Health**: Node status, leader election, consensus performance
- **VM Metrics**: Running VMs, health status, resource usage, migration statistics
- **Storage Metrics**: Ceph cluster health, OSD performance, replication status
- **Hypervisor Metrics**: KVM/Firecracker performance, VM density, migration latency
- **Network Metrics**: Virtual network performance, VLAN utilization, load balancer stats
- **Performance Metrics**: Operation latency, throughput, error rates

#### Integration with Monitoring Systems
```bash
# Prometheus integration (VMs + Ceph)
blixard metrics export --format prometheus --endpoint http://prometheus:9090
blixard storage metrics export --format prometheus --ceph-mgr-module

# Grafana dashboards
blixard dashboard generate --format grafana --type cluster > cluster-dashboard.json
blixard dashboard generate --format grafana --type storage > ceph-dashboard.json
blixard dashboard generate --format grafana --type vms > vm-dashboard.json

# Custom webhooks and alerting
blixard alerts configure --webhook https://alerts.example.com/webhook
blixard alerts rule --vm-cpu-high 80% --storage-full 90% --migration-failure
```

### Logging and Audit Trail

#### Structured Logging
```json
{
  "timestamp": "2024-12-09T15:30:00Z",
  "level": "INFO",
  "component": "consensus",
  "event": "service_start_proposed",
  "service_name": "nginx",
  "node_id": "node1",
  "raft_term": 15,
  "trace_id": "abc123"
}
```

#### Audit Trail
- **All Operations**: Complete record of all service management operations
- **Security Events**: Authentication, authorization, configuration changes
- **System Events**: Node joins/leaves, leader elections, failures
- **Compliance**: Immutable audit log for regulatory requirements

## ğŸ”’ Security and Access Control

### Authentication and Authorization

#### Role-Based Access Control (RBAC)
```bash
# Create roles
blixard rbac create-role service-operator \
  --permissions "service:start,service:stop,service:status"

blixard rbac create-role cluster-admin \
  --permissions "cluster:*,service:*,user:*"

# Assign roles to users
blixard rbac assign-role alice service-operator
blixard rbac assign-role bob cluster-admin
```

#### API Key Management
```bash
# Generate API keys for automation
blixard auth create-key --name ci-cd --role service-operator
blixard auth list-keys
blixard auth revoke-key ci-cd
```

### Network Security
- **mTLS**: All inter-node communication encrypted and authenticated
- **Tailscale Integration**: Secure mesh networking with identity-based access
- **Network Policies**: Control which services can communicate
- **Firewall Rules**: Automatic iptables integration for service ports

## ğŸ”„ Migration and Compatibility

### Migration from Current Gleam Version

#### Data Migration
```bash
# Export current service definitions
gleam run -m service_manager -- export --format json > services.json

# Import into Rust version
blixard import --format json --file services.json --verify

# Verify migration
blixard service list --compare-with services.json
```

#### Zero-Downtime Migration
1. **Parallel Deployment**: Run Rust version alongside Gleam version
2. **Service Migration**: Move services one by one to Rust cluster
3. **Validation**: Verify each service works correctly
4. **Cutover**: Switch CLI to point to Rust cluster
5. **Cleanup**: Decommission Gleam cluster

### Backward Compatibility
- **CLI Commands**: 100% compatible with existing scripts
- **Configuration Format**: Automatic conversion from old format
- **API Endpoints**: RESTful API maintains compatibility
- **Data Formats**: Automatic migration of stored data

## ğŸ¯ Success Criteria and KPIs

### Technical Success Metrics
- **Reliability**: 99.9%+ uptime in production environments
- **Performance**: 10x improvement in operation latency over Gleam version
- **Scalability**: Support for 100+ node clusters
- **Resource Efficiency**: 50% reduction in memory usage

### User Experience Metrics
- **Setup Time**: < 5 minutes to get first cluster running
- **Learning Curve**: Existing users can use new version immediately
- **Documentation**: Complete API documentation and tutorials
- **Community**: Active GitHub repository with regular releases

### Operational Metrics
- **Zero Security Incidents**: No consensus safety violations in production
- **Fast Recovery**: < 60 seconds MTTR for node failures
- **Efficient Updates**: Zero-downtime cluster upgrades
- **Comprehensive Monitoring**: Full observability into cluster health

## ğŸ›£ï¸ Roadmap and Future Features

### Phase 1: NixOS-Native MicroVM Platform (Months 1-3)
- âœ… NixOS module for Blixard service configuration
- âœ… microvm.nix deep integration for both long-lived and serverless VMs
- âœ… Firecracker integration optimized for <100ms cold starts
- âœ… Raft consensus for VM state with Nix store sharing
- âœ… Basic serverless function runtime and triggers
- âœ… Dynamic placement engine for serverless workloads
- âœ… Multi-node NixOS clustering with any-node execution
- âœ… CLI interface for both VMs and functions

### Phase 2: Serverless + Advanced Operations (Months 4-6)
- ğŸ”„ Advanced serverless features:
  - Event-driven triggers (HTTP, queues, streams, cron)
  - Warm pool optimization and instance recycling
  - Distributed tracing for function invocations
  - Auto-scaling policies and metrics
- ğŸ”„ Intelligent placement optimization:
  - ML-based placement predictions
  - Cross-region function deployment
  - Automatic data locality detection
- ğŸ”„ Live VM migration (long-lived VMs only)
- ğŸ”„ Advanced Ceph integration for function data
- ğŸ”„ Function composition and workflows
- ğŸ”„ Local development environment for functions
- ğŸ”„ A/B testing and canary deployments for functions

### Phase 3: Enterprise Features (Months 7-9)
- ğŸ“‹ RBAC and multi-tenant security
- ğŸ“‹ Advanced monitoring (VM metrics, Ceph telemetry)
- ğŸ“‹ Backup and disaster recovery
- ğŸ“‹ Cross-region Ceph replication
- ğŸ“‹ GPU and SR-IOV passthrough
- ğŸ“‹ Compliance and audit logging

### Phase 4: Ecosystem Integration (Months 10-12)
- ğŸ“‹ Kubernetes CRI integration (run pods in VMs)
- ğŸ“‹ Docker/Podman compatibility
- ğŸ“‹ Cloud provider integrations (AWS, GCP, Azure)
- ğŸ“‹ CI/CD pipeline integrations
- ğŸ“‹ Terraform/Pulumi providers
- ğŸ“‹ GitOps workflows

### Future Enhancements (NixOS-Focused)
- **Advanced Serverless**: 
  - WebAssembly runtime for polyglot functions
  - Streaming functions with stateful processing
  - GPU-accelerated serverless for AI inference
  - Cross-cluster function federation
- **Intelligent Scheduling**:
  - Predictive placement using cluster-wide telemetry
  - Carbon-aware scheduling (run on greenest nodes)
  - Cost-optimized placement for multi-cloud
- **Developer Experience**:
  - VS Code extension for function development
  - Instant function testing with hot reload
  - Distributed debugging across invocations
- **Edge Computing**: 
  - Ultra-low latency functions on edge nodes
  - Offline-capable serverless functions
  - IoT event processing at the edge
- **AI/ML Platform**:
  - Serverless training job orchestration
  - Model serving with automatic scaling
  - Federated learning across nodes
- **Nix Ecosystem**:
  - Function marketplace with Nix flakes
  - Pre-built function templates
  - Automatic dependency optimization

## ğŸ“š Documentation and Community

### Documentation Structure
```
docs/
â”œâ”€â”€ getting-started/
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ first-cluster.md
â”‚   â””â”€â”€ basic-operations.md
â”œâ”€â”€ user-guide/
â”‚   â”œâ”€â”€ service-management.md
â”‚   â”œâ”€â”€ cluster-operations.md
â”‚   â””â”€â”€ monitoring.md
â”œâ”€â”€ administrator-guide/
â”‚   â”œâ”€â”€ production-deployment.md
â”‚   â”œâ”€â”€ security.md
â”‚   â””â”€â”€ troubleshooting.md
â”œâ”€â”€ developer-guide/
â”‚   â”œâ”€â”€ api-reference.md
â”‚   â”œâ”€â”€ extending-blixard.md
â”‚   â””â”€â”€ contributing.md
â””â”€â”€ examples/
    â”œâ”€â”€ web-application.md
    â”œâ”€â”€ database-cluster.md
    â””â”€â”€ microservices.md
```

### Community and Support
- **GitHub Repository**: Open source with active issue tracking
- **Documentation Site**: Comprehensive guides and API reference
- **Community Forum**: User discussions and support
- **Regular Releases**: Monthly releases with new features and bug fixes
- **Professional Support**: Available for enterprise deployments

---

**Blixard represents the next generation of distributed service management, combining the reliability of battle-tested consensus algorithms with the performance and safety of modern Rust. Built from the ground up with testing, reliability, and operational excellence in mind, it provides a solid foundation for managing services in production environments.**